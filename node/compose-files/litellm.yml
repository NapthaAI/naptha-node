
services:
  litellm-ollama:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    command: --config /app/config.yaml
    restart: unless-stopped
    depends_on:
      pgvector:
        condition: service_healthy
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:?error}
      LITELLM_SALT_KEY: ${LITELLM_SALT_KEY:?error}
      DATABASE_URL: postgresql://${LOCAL_DB_POSTGRES_USERNAME?:error}:${LOCAL_DB_POSTGRES_PASSWORD:?error}@pgvector:5432/litellm
      OPENAI_API_KEY: ${OPENAI_API_KEY:?error}
    volumes:
      - ./node/inference/litellm/litellm_config.yml:/app/config.yaml
    ports:
      - '4000:4000' # TODO do not expose in production